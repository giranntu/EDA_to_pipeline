{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fe8b49",
   "metadata": {},
   "source": [
    "# Create & Deploy Vertex-AI Pipeline w/ Kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4141879",
   "metadata": {},
   "source": [
    "Install the needed libraries in order to run the code locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472ee279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install google-cloud-aiplatform==1.0.0 --upgrade --user\n",
    "# !pip install kfp --upgrade --user\n",
    "# !pip3 install kfp google-cloud-pipeline-components==0.1.1 --upgrade --user\n",
    "# !pip3 install scikit-learn --user\n",
    "# !pip3 install google-cloud-aiplatform --upgrade --user\n",
    "# !pip3 install pandas --user\n",
    "# !pip3 install python-dotenv --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5b11c",
   "metadata": {},
   "source": [
    "Might need to restart kernel after initial installation of the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d46d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, component, ClassificationMetrics, Metrics)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from typing import NamedTuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8b4a3",
   "metadata": {},
   "source": [
    "Getting some preset environment variables save to a local file. Create one of your own by following these instructions: https://stackoverflow.com/a/54028874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "341e1fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n",
      "cannot find .env file\n",
      "deep-learning-dlhlp\n",
      "deep-learning-dlhlp\n",
      "gs://deep-learning-dlhlp/pipeline_root\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/a/54028874\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "PROJECT_ID = \"deep-learning-dlhlp\" #os.environ['PROJECT_ID']\n",
    "BUCKET_NAME = \"deep-learning-dlhlp\" #os.environ['BUCKET']\n",
    "\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root'.format(BUCKET_NAME)\n",
    "REGION = 'us-central1'\n",
    "\n",
    "print(PROJECT_ID)\n",
    "print(BUCKET_NAME)\n",
    "print(PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75209539",
   "metadata": {},
   "source": [
    "## 1. Create a component for reading data from BQ into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ae69b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\", \"google-cloud-aiplatform\", \"google-cloud-bigquery-storage\",\"google-cloud-bigquery\",\"pyarrow\"], output_component_file=\"preprocess.yaml\")\n",
    "def preprocess(output_csv_path: OutputPath('CSV')):\n",
    "    #1\n",
    "    from google.cloud import bigquery\n",
    "    import google.auth\n",
    "    \n",
    "    creds, project = google.auth.default()\n",
    "    client = bigquery.Client(project='deep-learning-dlhlp', credentials=creds)\n",
    "\n",
    "    query =     \"\"\"\n",
    "            SELECT * FROM `deep-learning-dlhlp.telco.churn`\n",
    "    \"\"\"\n",
    "    print(query)\n",
    "    \n",
    "    dataframe = client.query(query).to_dataframe()\n",
    "    print(dataframe.head())\n",
    "    \n",
    "    dataframe.to_csv(output_csv_path)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd80aa1",
   "metadata": {},
   "source": [
    "## 2. Create a component to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b026b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\", \"imbalanced-learn\", \"google-cloud-aiplatform\", \"pyarrow\"])\n",
    "def train(wmetrics: Output[ClassificationMetrics], input_csv_path: InputPath('CSV'), saved_model: Output[Model], artifact_uri: OutputPath(str), accuracy: Output[Metrics], model_type: str, project_id: str, bucket: str):\n",
    "    from google.cloud import aiplatform\n",
    "    from typing import NamedTuple\n",
    "    #Train\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    print(len(df))\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype=='object':    #Since we are encoding object datatype to integer/float\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(df[c].values))\n",
    "            df[c] = lbl.transform(df[c].values)\n",
    "    print(df.head())  #To check if properly encoded\n",
    "    \n",
    "    X = df[['Contract', 'tenure', 'TechSupport', 'OnlineSecurity', 'TotalCharges', 'PaperlessBilling',\n",
    "       'DeviceProtection', 'Dependents', 'OnlineBackup', 'SeniorCitizen', 'MonthlyCharges',\n",
    "       'PaymentMethod', 'Partner', 'PhoneService']] #taking only relevant columns\n",
    "    y = df['Churn']\n",
    "\n",
    "\n",
    "    # Scaling all the variables to a range of 0 to 1\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    features = X.columns.values\n",
    "    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    scaler.fit(X)\n",
    "    X = pd.DataFrame(scaler.transform(X))\n",
    "    X.columns = features\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "\n",
    "    #Choose which model to train\n",
    "    if model_type == 'mlp':\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        #from sklearn.linear_model import LogisticRegression\n",
    "        model = MLPClassifier(alpha=1, max_iter=1000)\n",
    "        \n",
    "    elif model_type == 'random_forest':\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        model = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "        \n",
    "    elif model_type == 'decision_tree':\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        model = DecisionTreeClassifier()\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #Save the model to disk and also automatically to GCS\n",
    "    import joblib\n",
    "    \n",
    "    joblib.dump(model, os.path.join(saved_model.path.replace(\"saved_model\",\"\"), 'model.joblib'))\n",
    "    print(\" saved_model.path: \"+ saved_model.path)\n",
    "    print(\" saved_model.uri: \"+ saved_model.uri)\n",
    "    with open(artifact_uri, 'w') as f:\n",
    "        f.write(saved_model.uri.replace(\"saved_model\",\"\"))\n",
    "    \n",
    "    print(saved_model.uri)\n",
    "    \n",
    "    accuracy.log_metric('accuracy', model.score(X_test, y_test)) # 71\n",
    "    \n",
    "    if model_type == 'decision_tree':\n",
    "        #Adding roc curve\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "\n",
    "        y_scores = cross_val_predict(model, X_train, y_train, cv=3, method=\"predict_proba\")\n",
    "        fpr, tpr, thresholds = roc_curve(\n",
    "            y_true=y_train, y_score=y_scores[:, 1], pos_label=True\n",
    "        )\n",
    "        wmetrics.log_roc_curve(fpr, tpr, thresholds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d22088",
   "metadata": {},
   "source": [
    "## 3. Eval component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4823235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def evalaluation(baseline: float, accuracy: Input[Metrics], accuracy2: Input[Metrics], accuracy3: Input[Metrics]) -> bool:\n",
    "    isBetter = False\n",
    "    print(\"baseline is :\", str(baseline))\n",
    "    print(\"str(dir(accuracy)):, \", str(dir(accuracy)))\n",
    "    new_val = float(accuracy.metadata['accuracy'])\n",
    "    print(\"new_val is:\", str(new_val))\n",
    "    \n",
    "    if new_val>baseline:\n",
    "        isBetter = True\n",
    "    print(\"isBetter: \"+str(isBetter))\n",
    "    \n",
    "    return isBetter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e080464",
   "metadata": {},
   "source": [
    "## 4. Predict Endpoint component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "17f1f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/googleapis/python-aiplatform/blob/master/samples/snippets/predict_custom_trained_model_sample.py\n",
    "#https://cloud.google.com/ai-platform/prediction/docs/online-predict\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def predict_endpoint_test(endpoint_id: Input[Artifact],\n",
    "                          location: str,\n",
    "                          project: str,\n",
    "                          api_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\n",
    "    \n",
    "    from typing import Dict\n",
    "    from google.cloud import aiplatform\n",
    "    from google.protobuf import json_format\n",
    "    from google.protobuf.struct_pb2 import Value\n",
    "    \n",
    "    print(endpoint_id)\n",
    "    endpoint_id = endpoint_id.uri.split('/')[-1]\n",
    "    print(endpoint_id)\n",
    "    \n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    \n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    #https://machinelearningmastery.com/make-predictions-scikit-learn/\n",
    "    instance_dict = [ 1.74481176,  0.86540763, -1.07296862 ,-2.3015387,  -2.06014071, 1.46210794, 0.3190391 , -0.24937038 ,-0.61175641 ,-0.7612069 , -0.38405435, -0.52817175, -0.3224172,   1.62434536]\n",
    "    \n",
    "    instance = json_format.ParseDict(instance_dict, Value())\n",
    "    instances = [instance]\n",
    "    \n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    \n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        print(\" prediction:\" + str(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9f5dc67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4bc96d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"train-scikit\" + str(uuid.uuid4()))\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    bucket: str = BUCKET_NAME,\n",
    "    baseline_accuracy: float = 0.70\n",
    "):\n",
    "    preprocess_task = preprocess()\n",
    "    \n",
    "    train_task = train(preprocess_task.output, model_type='decision_tree', project_id=PROJECT_ID, bucket=BUCKET_NAME)\n",
    "    train_task2 = train(preprocess_task.output, model_type='random_forest', project_id=PROJECT_ID, bucket=BUCKET_NAME)\n",
    "    train_task3 = train(preprocess_task.output, model_type='mlp', project_id=PROJECT_ID, bucket=BUCKET_NAME)\n",
    "    \n",
    "    eval_task = evalaluation(baseline_accuracy, train_task.outputs[\"accuracy\"], train_task2.outputs[\"accuracy\"], train_task3.outputs[\"accuracy\"])\n",
    "    \n",
    "    with dsl.Condition(eval_task.output == \"true\", name=\"eval models\"):\n",
    "        model_upload_op = gcc_aip.ModelUploadOp(\n",
    "            project=PROJECT_ID,\n",
    "            display_name=\"model\"+TIMESTAMP, \n",
    "    #        artifact_uri=\"gs://user-group-demo/pipeline_root/141610882258/train-scikitf989f632-b955-4bb1-a72d-0480d1c08627-20210620145355/train_-6780204423378370560/\", # GCS location of model\n",
    "            artifact_uri=train_task.outputs[\"artifact_uri\"], # GCS location of model\n",
    "            serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\",\n",
    "        )\n",
    "\n",
    "        endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "            project=PROJECT_ID,\n",
    "            display_name=\"pipelines\"+TIMESTAMP,\n",
    "        )\n",
    "\n",
    "        model_deploy_op = gcc_aip.ModelDeployOp( \n",
    "            project=PROJECT_ID,\n",
    "            endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "            model=model_upload_op.outputs[\"model\"],\n",
    "            deployed_model_display_name=\"model_display_name\",\n",
    "            machine_type=\"n1-standard-4\",\n",
    "        )\n",
    "        \n",
    "        predict_task = predict_endpoint_test(project=PROJECT_ID, location=REGION, endpoint_id = model_deploy_op.outputs['endpoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "82d84daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, \n",
    "                            package_path=\"dag-\"+TIMESTAMP+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0202f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "32a01d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/train-scikita804c0e8-d998-490c-840b-6ffc3e9395c6-20210731102640?project=deep-learning-dlhlp\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    \"dag-\"+TIMESTAMP+\".json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a55fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09073b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f863e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ce730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8814193",
   "metadata": {},
   "source": [
    "# Create AutoML training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605024d4",
   "metadata": {},
   "source": [
    "Create a managed image dataset from a CSV file and train it using AutoML Tabular Training."
   ]
  },
  {
   "cell_type": "raw",
   "id": "88fae190",
   "metadata": {},
   "source": [
    "Classification (binary):\n",
    "     \"maximize-au-roc\" (default) - Maximize the area under the receiver\n",
    "                                 operating characteristic (ROC) curve.\n",
    "     \"minimize-log-loss\" - Minimize log loss.\n",
    "     \"maximize-au-prc\" - Maximize the area under the precision-recall curve.\n",
    "     \"maximize-precision-at-recall\" - Maximize precision for a specified\n",
    "                                     recall value.\n",
    "     \"maximize-recall-at-precision\" - Maximize recall for a specified\n",
    "                                     precision value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691fa2d3",
   "metadata": {},
   "source": [
    "Define the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88aa4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"automl-tab-training-v2\")\n",
    "def pipeline(project: str = PROJECT_ID):\n",
    "\n",
    "    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        project=project, display_name=\"churn-automl\", bq_source=[\"bq://deep-learning-dlhlp.telco.churn\"]  #gcs_source=gcs_csv_path\n",
    "    )\n",
    "\n",
    "    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n",
    "        project=project,\n",
    "        display_name=\"train-churn-automl_1\",\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        column_transformations=[            \n",
    "            {\"categorical\" : {\"column_name\": \"Contract\"}},\n",
    "            {\"numeric\" : {\"column_name\": \"tenure\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"TechSupport\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"OnlineSecurity\"}},\n",
    "            {\"numeric\" : {\"column_name\": \"TotalCharges\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"PaperlessBilling\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"DeviceProtection\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"Dependents\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"OnlineBackup\"}},\n",
    "            {\"numeric\" : {\"column_name\": \"SeniorCitizen\"}},\n",
    "            {\"numeric\" : {\"column_name\": \"MonthlyCharges\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"PaymentMethod\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"Partner\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"PhoneService\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"Churn\"}},\n",
    "        ],\n",
    "        optimization_objective=\"minimize-log-loss\",\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"Churn\",\n",
    "    )\n",
    "\n",
    "    deploy_op = gcc_aip.ModelDeployOp(  # noqa: F841\n",
    "        model=training_op.outputs[\"model\"],\n",
    "        project=project,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c939f44",
   "metadata": {},
   "source": [
    "# Compile and run the pipeline\n",
    "Now, you're ready to compile the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ac47026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"churn_classification_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb65a7",
   "metadata": {},
   "source": [
    "\n",
    "The pipeline compilation generates the tab_regression_pipeline.json job spec file.\n",
    "\n",
    "Next, instantiate an API client object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e69ea240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient  # noqa: F811\n",
    "\n",
    "api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d980b02d",
   "metadata": {},
   "source": [
    "Then, you run the defined pipeline like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f7a1781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/automl-tab-training-v2-20210731094257?project=deep-learning-dlhlp\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    \"churn_classification_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\"project\": PROJECT_ID},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5285d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text\n",
    "#categorical"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
