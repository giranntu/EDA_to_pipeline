{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa744dd5",
   "metadata": {},
   "source": [
    "# Create & Deploy Vertex-AI Pipeline w/ Kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91089de",
   "metadata": {},
   "source": [
    "Install the needed libraries in order to run the code locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c930d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install google-cloud-aiplatform==1.0.0 --upgrade --user\n",
    "# !pip install kfp --upgrade --user\n",
    "# !pip3 install kfp google-cloud-pipeline-components==0.1.1 --upgrade --user\n",
    "# !pip3 install scikit-learn --user\n",
    "# !pip3 install google-cloud-aiplatform --upgrade --user\n",
    "# !pip3 install pandas --user\n",
    "# !pip3 install python-dotenv --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360985a6",
   "metadata": {},
   "source": [
    "Might need to restart kernel after initial installation of the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805404dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, component, ClassificationMetrics, Metrics)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from typing import NamedTuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb11df2",
   "metadata": {},
   "source": [
    "Getting some preset environment variables save to a local file. Create one of your own by following these instructions: https://stackoverflow.com/a/54028874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f46ea-2f82-4da0-8e3e-920a6875c608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da865c75-2c18-4537-83e9-415aed976b08",
   "metadata": {},
   "source": [
    "# Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f729be-1f7d-40ec-8c5a-9498d53e707c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-01-c71acc52b320/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'qwiklabs-gcp-01-c71acc52b320' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n",
      "\n",
      "Project ID: qwiklabs-gcp-01-c71acc52b320\n",
      "Region: us-central1\n",
      "Bucket name: qwiklabs-gcp-01-c71acc52b320\n",
      "GS_BUCKET name: gs://qwiklabs-gcp-01-c71acc52b320\n",
      "Service Account: 400680520184-compute@developer.gserviceaccount.com\n",
      "Vertex API Parent URI: projects/qwiklabs-gcp-01-c71acc52b320/locations/us-central1\n",
      "Vertex PIPELINE_ROOT: gs://qwiklabs-gcp-01-c71acc52b320/pipeline_root\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = '[your-project-id]' # Change to your project id.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "BUCKET_NAME = '[your-bucket-name]'  # Change to your bucket name.\n",
    "SERVICE_ACCOUNT = \"[your-service-account]\"\n",
    "\n",
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    # Get your bucket name to GCP projet id\n",
    "    BUCKET_NAME = PROJECT_ID\n",
    "    # Try to create the bucket if it doesn'exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET_NAME\n",
    "    print(\"\")\n",
    "    \n",
    "PARENT = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "GS_BUCKET = 'gs://{}'.format(BUCKET_NAME)\n",
    "\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root'.format(BUCKET_NAME)\n",
    "\n",
    "\n",
    "print(\"Project ID:\", PROJECT_ID)\n",
    "print(\"Region:\", REGION)\n",
    "print(\"Bucket name:\", BUCKET_NAME)\n",
    "print(\"GS_BUCKET name:\", GS_BUCKET)\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)\n",
    "print(\"Vertex API Parent URI:\", PARENT)\n",
    "print(\"Vertex PIPELINE_ROOT:\", PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118cdf9c",
   "metadata": {},
   "source": [
    "## 1. Create a component for reading data from BQ into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d06c1a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\", \"google-cloud-aiplatform\", \"google-cloud-bigquery-storage\",\"google-cloud-bigquery\",\"pyarrow\"], output_component_file=\"preprocess.yaml\")\n",
    "def preprocess(output_csv_path: OutputPath('CSV')):\n",
    "    #1\n",
    "    from google.cloud import bigquery\n",
    "    import google.auth\n",
    "    \n",
    "    creds, project = google.auth.default()\n",
    "    client = bigquery.Client(project='qwiklabs-gcp-01-c71acc52b320', credentials=creds)\n",
    "\n",
    "    query =     \"\"\"\n",
    "            SELECT * FROM `qwiklabs-gcp-01-c71acc52b320.churn.churn_data`\n",
    "    \"\"\"\n",
    "    print(query)\n",
    "    \n",
    "    dataframe = client.query(query).to_dataframe()\n",
    "    print(dataframe.head())\n",
    "    \n",
    "    dataframe.to_csv(output_csv_path)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0410e",
   "metadata": {},
   "source": [
    "## 2. Create a component to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3784a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\", \"imbalanced-learn\", \"google-cloud-aiplatform\", \"pyarrow\"])\n",
    "def train(wmetrics: Output[ClassificationMetrics], input_csv_path: InputPath('CSV'), saved_model: Output[Model], artifact_uri: OutputPath(str), accuracy: Output[Metrics],\n",
    "          model_type: str, project_id: str, bucket: str, split:float ):\n",
    "    from google.cloud import aiplatform\n",
    "    from typing import NamedTuple\n",
    "    \n",
    "    import pandas as pd\n",
    "    # FIXME\n",
    "    # import os\n",
    "    \n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    print(len(df))\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype=='object':    #Since we are encoding object datatype to integer/float\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(df[c].values))\n",
    "            df[c] = lbl.transform(df[c].values)\n",
    "    print(df.head())  #To check if properly encoded\n",
    "    \n",
    "    X = df[['Contract', 'tenure', 'TechSupport', 'OnlineSecurity', 'TotalCharges', 'PaperlessBilling',\n",
    "       'DeviceProtection', 'Dependents', 'OnlineBackup', 'SeniorCitizen', 'MonthlyCharges',\n",
    "       'PaymentMethod', 'Partner', 'PhoneService']] #taking only relevant columns\n",
    "    y = df['Churn']\n",
    "\n",
    "\n",
    "    # Scaling all the variables to a range of 0 to 1\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    features = X.columns.values\n",
    "    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    scaler.fit(X)\n",
    "    X = pd.DataFrame(scaler.transform(X))\n",
    "    X.columns = features\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=101)\n",
    "\n",
    "    #Choose which model to train\n",
    "    if model_type == 'mlp':\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        #from sklearn.linear_model import LogisticRegression\n",
    "        model = MLPClassifier(alpha=1, max_iter=1000)\n",
    "        \n",
    "    elif model_type == 'random_forest':\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        model = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "        \n",
    "    elif model_type == 'decision_tree':\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        model = DecisionTreeClassifier()\n",
    "        \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #Save the model to disk and also automatically to GCS\n",
    "    import joblib\n",
    "    \n",
    "    joblib.dump(model, os.path.join(saved_model.path.replace(\"saved_model\",\"\"), 'model.joblib'))\n",
    "    print(\" saved_model.path: \"+ saved_model.path)\n",
    "    print(\" saved_model.uri: \"+ saved_model.uri)\n",
    "    with open(artifact_uri, 'w') as f:\n",
    "        f.write(saved_model.uri.replace(\"saved_model\",\"\"))\n",
    "    \n",
    "    print(saved_model.uri)\n",
    "    \n",
    "    accuracy.log_metric('accuracy', model.score(X_test, y_test)) # 71\n",
    "    \n",
    "    if model_type == 'decision_tree':\n",
    "        #Adding roc curve\n",
    "        from sklearn.metrics import roc_curve\n",
    "        from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "\n",
    "        y_scores = cross_val_predict(model, X_train, y_train, cv=3, method=\"predict_proba\")\n",
    "        fpr, tpr, thresholds = roc_curve(\n",
    "            y_true=y_train, y_score=y_scores[:, 1], pos_label=True\n",
    "        )\n",
    "        wmetrics.log_roc_curve(fpr, tpr, thresholds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f3557a",
   "metadata": {},
   "source": [
    "## 3. Eval component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4333db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def evalaluation(baseline: float, accuracy: Input[Metrics], accuracy2: Input[Metrics], accuracy3: Input[Metrics]) -> bool:\n",
    "    isBetter = False\n",
    "    print(\"baseline is :\", str(baseline))\n",
    "    print(\"str(dir(accuracy)):, \", str(dir(accuracy)))\n",
    "    new_val = float(accuracy.metadata['accuracy'])\n",
    "    print(\"new_val is:\", str(new_val))\n",
    "    \n",
    "    if new_val>baseline:\n",
    "        isBetter = True\n",
    "    print(\"isBetter: \"+str(isBetter))\n",
    "    \n",
    "    return isBetter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d232ff0",
   "metadata": {},
   "source": [
    "## 4. Predict Endpoint component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01e60081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/googleapis/python-aiplatform/blob/master/samples/snippets/predict_custom_trained_model_sample.py\n",
    "#https://cloud.google.com/ai-platform/prediction/docs/online-predict\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def predict_endpoint_test(endpoint_id: Input[Artifact],\n",
    "                          location: str,\n",
    "                          project: str,\n",
    "                          api_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\n",
    "    \n",
    "    from typing import Dict\n",
    "    from google.cloud import aiplatform\n",
    "    from google.protobuf import json_format\n",
    "    from google.protobuf.struct_pb2 import Value\n",
    "    \n",
    "    print(endpoint_id)\n",
    "    endpoint_id = endpoint_id.uri.split('/')[-1]\n",
    "    print(endpoint_id)\n",
    "    \n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    \n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    #https://machinelearningmastery.com/make-predictions-scikit-learn/\n",
    "    instance_dict = [ 1.74481176,  0.86540763, -1.07296862 ,-2.3015387,  -2.06014071, 1.46210794, 0.3190391 , -0.24937038 ,-0.61175641 ,-0.7612069 , -0.38405435, -0.52817175, -0.3224172,   1.62434536]\n",
    "    \n",
    "    instance = json_format.ParseDict(instance_dict, Value())\n",
    "    instances = [instance]\n",
    "    \n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    \n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        print(\" prediction:\" + str(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "643b3c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "416dee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"train-scikit\" + str(uuid.uuid4()))\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    bucket: str = BUCKET_NAME,\n",
    "    baseline_accuracy: float = 0.70,\n",
    "    test_split_size: float = 0.1\n",
    "):\n",
    "    preprocess_task = preprocess()\n",
    "    \n",
    "    train_task = train(preprocess_task.output, model_type='decision_tree', project_id=PROJECT_ID, bucket=BUCKET_NAME, split = test_split_size)\n",
    "    train_task2 = train(preprocess_task.output, model_type='random_forest', project_id=PROJECT_ID, bucket=BUCKET_NAME, split = test_split_size)\n",
    "    train_task3 = train(preprocess_task.output, model_type='mlp', project_id=PROJECT_ID, bucket=BUCKET_NAME, split = test_split_size)\n",
    "    \n",
    "    eval_task = evalaluation(baseline_accuracy, train_task.outputs[\"accuracy\"], train_task2.outputs[\"accuracy\"], train_task3.outputs[\"accuracy\"])\n",
    "    \n",
    "    with dsl.Condition(eval_task.output == \"true\", name=\"eval models\"):\n",
    "        model_upload_op = gcc_aip.ModelUploadOp(\n",
    "            project=PROJECT_ID,\n",
    "            display_name=\"model\"+TIMESTAMP, \n",
    "    #        artifact_uri=\"gs://user-group-demo/pipeline_root/141610882258/train-scikitf989f632-b955-4bb1-a72d-0480d1c08627-20210620145355/train_-6780204423378370560/\", # GCS location of model\n",
    "            artifact_uri=train_task.outputs[\"artifact_uri\"], # GCS location of model\n",
    "            serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\",\n",
    "        )\n",
    "\n",
    "        endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "            project=PROJECT_ID,\n",
    "            display_name=\"pipelines\"+TIMESTAMP,\n",
    "        )\n",
    "\n",
    "        model_deploy_op = gcc_aip.ModelDeployOp( \n",
    "            project=PROJECT_ID,\n",
    "            endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "            model=model_upload_op.outputs[\"model\"],\n",
    "            deployed_model_display_name=\"model_display_name\",\n",
    "            machine_type=\"n1-standard-4\",\n",
    "        )\n",
    "        \n",
    "        predict_task = predict_endpoint_test(project=PROJECT_ID, location=REGION, endpoint_id = model_deploy_op.outputs['endpoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef62602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, \n",
    "                            package_path=\"dag-\"+TIMESTAMP+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d4508bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f71962a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/train-scikitb4922fd8-6ca7-405f-ae5f-a67fd16124e7-20211008044230?project=qwiklabs-gcp-01-c71acc52b320\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    \"dag-\"+TIMESTAMP+\".json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c180321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea2a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815adf7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f2539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d832f019",
   "metadata": {},
   "source": [
    "# Create AutoML training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d4db8",
   "metadata": {},
   "source": [
    "Create a managed image dataset from a CSV file and train it using AutoML Tabular Training."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9abf336d",
   "metadata": {},
   "source": [
    "Classification (binary):\n",
    "     \"maximize-au-roc\" (default) - Maximize the area under the receiver\n",
    "                                 operating characteristic (ROC) curve.\n",
    "     \"minimize-log-loss\" - Minimize log loss.\n",
    "     \"maximize-au-prc\" - Maximize the area under the precision-recall curve.\n",
    "     \"maximize-precision-at-recall\" - Maximize precision for a specified\n",
    "                                     recall value.\n",
    "     \"maximize-recall-at-precision\" - Maximize recall for a specified\n",
    "                                     precision value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4facb2c8",
   "metadata": {},
   "source": [
    "Define the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ba1067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"automl-tab-training-v2\")\n",
    "def pipeline(project: str = PROJECT_ID):\n",
    "\n",
    "    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        project=project, display_name=\"churn-automl\", bq_source=[\"bq://deep-learning-dlhlp.telco.churn\"]  #gcs_source=gcs_csv_path\n",
    "    )\n",
    "\n",
    "    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n",
    "        project=project,\n",
    "        display_name=\"train-churn-automl_1\",\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        column_transformations=[            \n",
    "            {\"categorical\" : {\"column_name\": \"Contract\"}},\n",
    "            {\"numeric\" : {\"column_name\": \"tenure\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"TechSupport\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"OnlineSecurity\"}},\n",
    "            {\"numeric\" : {\"column_name\": \"TotalCharges\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"PaperlessBilling\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"DeviceProtection\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"Dependents\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"OnlineBackup\"}},\n",
    "            {\"numeric\" : {\"column_name\": \"SeniorCitizen\"}},\n",
    "            {\"numeric\" : {\"column_name\": \"MonthlyCharges\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"PaymentMethod\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"Partner\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"PhoneService\"}},\n",
    "            {\"categorical\" : {\"column_name\": \"Churn\"}},\n",
    "        ],\n",
    "        optimization_objective=\"minimize-log-loss\",\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"Churn\",\n",
    "    )\n",
    "\n",
    "    deploy_op = gcc_aip.ModelDeployOp(  # noqa: F841\n",
    "        model=training_op.outputs[\"model\"],\n",
    "        project=project,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333fb33",
   "metadata": {},
   "source": [
    "# Compile and run the pipeline\n",
    "Now, you're ready to compile the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23788957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"churn_classification_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8f2b3",
   "metadata": {},
   "source": [
    "\n",
    "The pipeline compilation generates the tab_regression_pipeline.json job spec file.\n",
    "\n",
    "Next, instantiate an API client object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f240f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient  # noqa: F811\n",
    "\n",
    "api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c827214",
   "metadata": {},
   "source": [
    "Then, you run the defined pipeline like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54540e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/automl-tab-training-v2-20211008042741?project=qwiklabs-gcp-01-c71acc52b320\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    \"churn_classification_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\"project\": PROJECT_ID},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f23399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text\n",
    "#categorical"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
